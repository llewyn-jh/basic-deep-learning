{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interracial-saint",
   "metadata": {},
   "source": [
    "# Set jupyter notebook autocomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-quilt",
   "metadata": {},
   "source": [
    "# Load IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_all, y_train_all), (x_test, y_test) = imdb.load_data(skip_top=20, num_words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-orbit",
   "metadata": {},
   "source": [
    "# Get rid of digit 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train_all)):\n",
    "    x_train_all[i] = [n for n in x_train_all[i] if n > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-principle",
   "metadata": {},
   "source": [
    "# Transform digits to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = imdb.get_word_index()\n",
    "index2word = {word2index[k]: k for k in word2index}\n",
    "for w in x_train_all[0]:\n",
    "    print(index2word[w - 3], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-restriction",
   "metadata": {},
   "source": [
    "# Split train dataset and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_index = np.random.permutation(25000)\n",
    "\n",
    "x_train = x_train_all[random_index[:20000]]\n",
    "y_train = y_train_all[random_index[:20000]]\n",
    "x_val = x_train_all[random_index[20000:]]\n",
    "y_val = y_train_all[random_index[20000:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-spending",
   "metadata": {},
   "source": [
    "# Set a length of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-tokyo",
   "metadata": {},
   "source": [
    "# Encoding all sample by using one hot encoding of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_onehot = to_categorical(x_train)\n",
    "x_val_onebot = to_categorical(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-gazette",
   "metadata": {},
   "source": [
    "# Build a recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetwork:\n",
    "    \n",
    "    def __init__(self, n_cells=10, batch_size=32, learning_rate=0.1):\n",
    "        self.n_cells = n_cells\n",
    "        self.batch_size = batch_size\n",
    "        self.w1h = None\n",
    "        self.w1x = None\n",
    "        self.b1 = None\n",
    "        self.w2 = None\n",
    "        self.b2 = None\n",
    "        self.h = None\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def init_weights(self, n_features, n_classes):\n",
    "        orth_init = tf.initializers.Orthogonal()\n",
    "        glorot_init = tf.initializers.GlorotUniform()\n",
    "        \n",
    "        self.w1h = orth_init((self.n_cells, self.n_cells)).numpy()\n",
    "        self.w1x = glorot_init((n_features, self.n_cells)).numpy()\n",
    "        self.b1 = np.zeros(self.n_cells)\n",
    "        self.w2 = glorot_init((self.n_cells, n_classes)).numpy()\n",
    "        self.b2 = np.zeros(n_classes)\n",
    "        \n",
    "    def forpass(self, x):\n",
    "        self.h = [np.zeros((x.shape[0], self.n_cells))]\n",
    "        seq = np.swapaxes(x, 0, 1)\n",
    "        for x in seq:\n",
    "            z1 = np.dot(x, self.w1x) + np.dot(self.h[-1], self.w1h) + self.b1\n",
    "            h = np.tanh(z1)\n",
    "            self.h.append(h)\n",
    "            z2 = np.dot(h, self.w2) + self.b2\n",
    "        return z2\n",
    "    \n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)\n",
    "        \n",
    "        w2_grad = np.dot(self.h[-1], err) / m\n",
    "        b2_grad = np.sum(err) / m\n",
    "        \n",
    "        seq = np.swapaxes(x, 0, 1)\n",
    "        w1h_grad = w1x_grad = b1_grad = 0\n",
    "        err2cell = np.dot(err, self.w2.T) * (1 - self.h[-1] ** 2)\n",
    "        \n",
    "        for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):\n",
    "            w1h_grad += np.dot(h.T, err2cell)\n",
    "            w1x_grad += np.dot(x.T, err2cell)\n",
    "            b1_grad += np.sum(err2cell, axis=0)\n",
    "            err2cell = np.dot(err2cell, self.w1h) * (1 - h ** 2)\n",
    "            \n",
    "        w1h_grad /= m\n",
    "        w1x_grad /= m\n",
    "        b1_grad /= m\n",
    "        \n",
    "        return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        np.random.seed(42)\n",
    "        self.init_weights(x.shape[2], y.shape[1])\n",
    "        for i in range(epochs):\n",
    "            print('Epoch', i, end=' ')\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in self.gen_batch(x, y):\n",
    "                print('.', end='')\n",
    "                a = self.training(x_batch, y_batch)\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                loss = np.mean(-(y_batch * np.log(a) + (1 - y_batch) * np.log(1 - a)))\n",
    "                batch_losses.append(loss)\n",
    "            print()\n",
    "            self.losses.append(np.mean(batch_losses))\n",
    "            self.update_val_losses(x_val, y_val)\n",
    "            \n",
    "    def gen_batch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size\n",
    "        if length % self.batch_size:\n",
    "            bins += 1\n",
    "        indexes = np.random.permutation(np.arange(len(x)))\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i\n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]\n",
    "            \n",
    "    def training(self, x, y):\n",
    "        m = len(x)\n",
    "        z = self.forpass(x)\n",
    "        a = self.sigmoid(z)\n",
    "        err = - (y - a)\n",
    "        w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        self.w1h -= self.lr * w1h_grad \n",
    "        self.w1x -= self.lr * w1x_grad\n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        self.w2 -= self.lr * w2_grad\n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a\n",
    "    \n",
    "    def update_val_losses(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)\n",
    "        a = self.sigmoid(z)\n",
    "        a = np.clip(a, 1e-10, 1-1e-10)\n",
    "        loss = np.mean(-(y_val * np.log(a) + (1 - y_val) * np.log(1 - a)))\n",
    "        self.val_losses.append(loss)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)\n",
    "        a = self.sigmoid(z)\n",
    "        return a > 0.5\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x) == y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-telephone",
   "metadata": {},
   "source": [
    "# Train a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-railway",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn = RecurrentNetwork(n_cells=32, batch_size=32, learning_rate=0.01)\n",
    "rnn.fit(x_train_onehot, y_train, epochs=20, x_val=x_val_onebot, y_val=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-forge",
   "metadata": {},
   "source": [
    "# Display losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rnn.losses)\n",
    "plt.plot(rnn.val_losses)\n",
    "plt.legend(['train', 'val'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-banner",
   "metadata": {},
   "source": [
    "# Check a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = rnn.score(x_val_onebot, y_val)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
